{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy, networkx, pandas\n",
    "\n",
    "# import graph_tool\n",
    "# from graph_tool.all import *\n",
    "\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# simple parallelization\n",
    "\n",
    "# import multiprocessing\n",
    "# cpus = multiprocessing.cpu_count()\n",
    "# pool = multiprocessing.Pool(processes=cpus)\n",
    "# pool.map(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def randomFailure(graph, steps=101):\n",
    "    initialGraph = graph\n",
    "    initialGraphSize = networkx.number_of_nodes(initialGraph)\n",
    "    numbersOfNodesToRemove = numpy.linspace(0, initialGraphSize, num=steps, dtype='int')\n",
    "    initialNodes = initialGraph.nodes()\n",
    "    randomizedNodes = numpy.random.permutation(initialNodes)\n",
    "    \n",
    "    def analyzeSingleGraph(index):\n",
    "        # TODO vedere se si possono agevolmente parallelizzare le list comprehension\n",
    "        # che sono molto più scorrevoli da usare ripetto a map() \n",
    "        newGraph = initialGraph.copy()\n",
    "        newGraph.remove_nodes_from(randomizedNodes[0:index])\n",
    "        newGraphSize = networkx.number_of_nodes(newGraph)\n",
    "        grado = newGraph.degree().items()\n",
    "        # subgraphs = sorted(networkx.connected_component_subgraphs(newGraph), key = len, reverse=True)\n",
    "        subgraphs = networkx.connected_component_subgraphs(newGraph)\n",
    "        try:\n",
    "            # giantCluster = subgraphs[0]\n",
    "            giantCluster = max(subgraphs, key = networkx.number_of_nodes)\n",
    "            giantClusterSize = networkx.number_of_nodes(giantCluster)\n",
    "            relativeGiantClusterSize = numpy.true_divide(giantClusterSize, newGraphSize)\n",
    "#            diameter = networkx.diameter(giantCluster, e=None)\n",
    "            diameter = 0\n",
    "        except:\n",
    "            giantCluster = networkx.empty_graph()\n",
    "            giantClusterSize = 0\n",
    "            relativeGiantClusterSize = 0\n",
    "            diameter = 0\n",
    "        return relativeGiantClusterSize, diameter\n",
    "    \n",
    "    # TODO parallelizzare questa mappa\n",
    "    failureResults = map(analyzeSingleGraph, numbersOfNodesToRemove)\n",
    "    failureDataframe = pandas.DataFrame(failureResults, columns=['relativeGiantClusterSize', 'diameter'])\n",
    "    ascisse = numpy.linspace(0,100, num=steps, dtype='int')\n",
    "    failureDataframe['percentuale'] = ascisse\n",
    "    \n",
    "    return failureDataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intentional attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def intentionalAttack(graph, steps=101):\n",
    "    initialGraph = graph\n",
    "    initialGraphSize = networkx.number_of_nodes(initialGraph)\n",
    "    numbersOfNodesToRemove = numpy.linspace(0, initialGraphSize, num=steps, dtype='int')\n",
    "    initialNodes = initialGraph.nodes()\n",
    "    \n",
    "    initialDegrees = initialGraph.degree()\n",
    "    degreeDataframe = pandas.DataFrame(initialDegrees.items(), columns=['ID', 'degree'])\n",
    "    degreeDataframe.sort([\"degree\"], ascending=[False], inplace=True) # TODO vedere se si può fare a meno di una colonna\n",
    "    # degreeDataframe = degreeDataframe.reset_index(drop=True)\n",
    "    sortedNodes = degreeDataframe['ID'].values # TODO degreeDataframe.ID\n",
    "    \n",
    "    def analyzeSingleGraph(number):\n",
    "        # TODO vedere se si possono agevolmente parallelizzare le list comprehension\n",
    "        # che sono molto più scorrevoli da usare ripetto a map() \n",
    "        newGraph = initialGraph.copy()\n",
    "        newGraph.remove_nodes_from(sortedNodes[0:number]) # TODO vedere ordinamento più veloce\n",
    "        newGraphSize = networkx.number_of_nodes(newGraph)\n",
    "        grado = newGraph.degree().items()\n",
    "        # subgraphs = sorted(networkx.connected_component_subgraphs(newGraph), key = len, reverse=True)\n",
    "        subgraphs = networkx.connected_component_subgraphs(newGraph)\n",
    "        try:\n",
    "            # giantCluster = subgraphs[0]\n",
    "            giantCluster = max(subgraphs, key = networkx.number_of_nodes)\n",
    "            giantClusterSize = networkx.number_of_nodes(giantCluster)\n",
    "            relativeGiantClusterSize = numpy.true_divide(giantClusterSize, newGraphSize)\n",
    "#            diameter = networkx.diameter(giantCluster, e=None)\n",
    "            diameter = 0\n",
    "        except:\n",
    "            giantCluster = networkx.empty_graph()\n",
    "            giantClusterSize = 0\n",
    "            relativeGiantClusterSize = 0\n",
    "            diameter = 0\n",
    "        return relativeGiantClusterSize, diameter\n",
    "    \n",
    "    # TODO parallelizzare questa mappa\n",
    "    attackResults = map(analyzeSingleGraph, numbersOfNodesToRemove)\n",
    "    attackDataframe = pandas.DataFrame(attackResults, columns=['relativeGiantClusterSize', 'diameter'])\n",
    "    ascisse = numpy.linspace(0,100, num=steps, dtype='int')\n",
    "    attackDataframe['percentuale'] = ascisse\n",
    "    \n",
    "    return attackDataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#gestori = [\"Tim\", \"Vodafone\", \"Wind\", \"Tre\", \"Roma\"]\n",
    "#colori = ['#004184','#ff3300','#ff8000','#018ECC', '#4d4d4d']\n",
    "\n",
    "#gestori = [\"Tim\", \"Vodafone\", \"Wind\", \"Tre\"]\n",
    "#colori = ['#004184','#ff3300','#ff8000','#018ECC']\n",
    "\n",
    "gestori = [\"Roma\"]\n",
    "colori = ['#004184']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roma random failure:\n",
      "CPU times: user 1min 50s, sys: 1.35 s, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data reading, calculations, data writing\n",
    "\n",
    "# TODO parallelizzare\n",
    "for provider in gestori:\n",
    "    \n",
    "    # read data\n",
    "    adjacencyMatrix = numpy.genfromtxt((\"/home/protoss/Documenti/Siscomp_datas/data/AdiacenzaEuclidea_{0}.csv\".format(provider)),\n",
    "                                 delimiter=',',\n",
    "                                 dtype='int')\n",
    "    providerGraph = networkx.Graph(adjacencyMatrix)\n",
    "    \n",
    "    # calculate results\n",
    "    print provider, \"random failure:\"\n",
    "    %time failureResults = randomFailure(providerGraph, steps=10) # default: steps=101\n",
    "#    print provider, \"intentional attack:\"\n",
    "#    %time attackResults = intentionalAttack(providerGraph, steps=101)\n",
    "    \n",
    "    # write on file\n",
    "    failureResults.to_csv('../data/percolation/ComparazioneRandom{0}.csv'.format(provider), index=False)\n",
    "#    attackResults.to_csv('../data/percolation/intentionalAttack_{0}.csv'.format(provider), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
